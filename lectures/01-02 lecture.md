14/02/2025

# Steps

## 1. Run omp 

```cpp
#include <stdio.h>

int main(void) {
#pragma omp parallel
    {
        printf("Hello\n");
    }
    
    return 0; 
}
```

По-умолчанию OMP создает столько потоков, сколько ядер в системе.

## 2. Thread_info

Задача -- каждый процесс обрабатывает свою часть массива (нужно знать, сколько тредов и номер треда).

```cpp
#include <stdio.h>
#include <omp.h>

int main(void) {
#pragma omp parallel
    {
        int n = omp_get_num_threads();
        int m = omp_get_thread_num();
        printf("Hello %i/%i\n", m, n);
    }
    
    return 0; 
}
```

Менеджмент тредов происходит на уровне ОС. Часть потоков может ожидать момента отправки на исполнение, поэтому нельзя ожидать, что треды стартанут одновременно

Поэтому для минимизации рандома при измерении времени надо убить весь шум. Плюс есть вопрос частоты процессора (особенно у моббильных процов), нужно втыкаться в зарядку против троттлинга.

## 3. Loops

```cpp
for (uint i = a; i < b; i++) {
    f(i); 
}
```

Мы хотим разбить этот цикл равномерно между n тредами и чтобы каждая итерация цикла обязательно в единственном экземпляре досталась какому-то треду

Положим, m = 4, b = 12 -- тогда каждому треду должна достаться обработка ровно 3 любых индексов i

Если b = 11, то три треда делают 3 итерации, один -- 1 итерация. То есть количество итераций отличаться должно не более чем на 1. 

Option 1
```cpp
#include <stdio.h>
#include <omp.h>

int main(void) {
#pragma omp parallel
    {
        int n = omp_get_num_threads();
        int m = omp_get_thread_num();

        int a = 0;
        int b = 100;

        for (int i = m; i < b; i+=n) {
            printf("Hello %d (current thread -- %d)\n", i, m);
        }
    }
    
    return 0; 
}
```

Second options (doesn't work yet):
! Важно отслеживать, чтобы не было переполнения при умножении
$$S=a+(b-a)*m/n$$
$$S=a+(b-a)*(m+1)/n$$

```cpp
#include <stdio.h>
#include <omp.h>

int main(void) {
#pragma omp parallel
    {
        int n = omp_get_num_threads();
        int m = omp_get_thread_num();

        int a = 0;
        int b = 101;

        int width = b / n;
        int num_long = b / m;

        int left = a;
        int right = b;

        if (n < num_long) {
            left = a + m * (width + 1);
            right = left + (width + 1);
        } else if (n == num_long) {
            left = a + m * (width + 1);
            right = left + width;
        } else {
            left = a + m * (width + 1) + n - num_long;
            right = left + width;
        }
        
        for (int i = left; i < right; i+=1) {
            printf("Hello %d (current thread -- %d)\n", i, m);
        }
    }
    
    return 0; 
}
```

Выбирать один или другой вариант надо на основе решаемой задачи.

У каждого ядра свой кэш, как правило.
Кэширование происходит блоками по 64 байта (например) и тогда разбиение через i+=x приведет к тому, что мы убьем кэширование. Поэтому выгодно разбиение вторым подходом.

Первый вариант выгоден, когда задачи разных размеров (например, внутренний цикл зависит от внешнего линейно).

## 4. Open MP loops

Open MP делает разрезание автоматом.


```cpp
#include <stdio.h>
#include <omp.h>

int main(void) {
#pragma omp parallel
    {
        int n = omp_get_num_threads();
        int m = omp_get_thread_num();

        int b = 100;

        #pragma omp for
        for (int i = 0; i < b; i+=1) {
            printf("Hello %d (current thread -- %d)\n", i, m);
        }
    }
    
    return 0; 
}
```

### Static

По дефолту делает так, как задано в текущей версии OpenMP
Если хотим поменять, то
Для второго варианта (12, 34, 56)
```cpp
#pragma omp for schedule(static)
```

Для первого варианта (цифра -- размер блоков)
```cpp
#pragma omp for schedule(static, 1)
```

Static значит, что распределение задач по тредам происходит перед циклом

### Dynamic

Альтернатива -- каждый тред приходит к некому механизму распределение и говорит "дай мне следующую итерацию цикла"

```cpp
#pragma omp for schedule(dynamic)
```

Приемущества dynamic:
Тут важно заметить, что часть тредов будут заняты чем-то другим в момент запуска программы, поэтому выгодно дать единственному активному треду все задачи по порядку и только при появлении других начать распределять.
Иными словами, балансировка максимально равномерна, так как треды не занимают индексы. Плюс неизвестно, какие задачи внезапно будут долгими и если одна будет существенно больше других, то выгодно в это время не давать треду, выполняющему ее другие индексы.

Проблемы dynamic:
Распределитель стоит дополнительных вычислений.

То есть если внутренности цикла большие, dynamic -- бомба.
Но если внутренности очень маленькие (a+b), то стоимость распределения будет заметна! И выгодно static

// TODO: command to allow threads to leave without waiting
> Чтобы выйти из цикла, потоки дожидаются, если не указать выход из цикла!

Можно балансировать по группам индексов, чтобы эффективно исопльзовать dynamic (k -- количество индексов в группе для понижения overhead)
```cpp
#pragma omp for schedule(dynamic, k)
```

> Нынче начинают несиметричные системы -- некоторые мощнее, некоторые слабее (последние intel процы). Поэтому dynamic тут опять выгоден.


### Задача

```
Простые числа
Делимость на все числа от 2 до корня квадратного от этого числа

Внешний цикл по числам
Внутренний цикл проверяет число на простоту
```

Решение без потоков:

```cpp
#include <stdio.h>
#include <iostream>
#include <omp.h>
#include <math.h>

bool isPrime(int number) {
    int root = sqrt(number);
    for (int i = 2; i <= root; i++) {
        if (number % i == 0) return false;
    }
    return true;
}

int main(void) {
    // дано n -- вывести число простых чисел <= n
    int n, counter{0};
    std::cin >> n;
    double time_before = omp_get_wtime();
    for (int i = 2; i <= n; i++) {
        if (isPrime(i)) counter++;
    }
    double time_after = omp_get_wtime();
    double time_diff = time_after - time_before;
    std::cout << counter << std::endl;
    std::cout << time_diff << std::endl;
}
```

Измерение времени:
```cpp
omp_get_wtime() //писать в double
```

В многотредовых программах важно, какие переменные у всех тредов, а какие отдельны для каждого треда. При использовании parallel можно указать, какие переменные копировать, а какие копировать в частные для каждого треда.
По дефолту -- все переменные вне #pragma omp parallel будуд общие, а внутри -- собственые для каждого треда.

```cpp
int y = 0; // Общие -- для каждого
#pragma omp parallel
{
    int x = 0; // Индивидуально для каждого
}
```

in: 100000000
out: 5761455
time: 81.1298


> Какой из циклов разрехаем на потоки??

Ответ -- внешний, аргументы:
 - Внутренний находится внутри внешнего -- то потоки создаются каждую итерацию -- плохо (создание и завершение -- дорого)
 - Если часть цикла не нужна, то при разделении заранее -- проверено будет все (break будет бесполезен в середни выполнения цикла, как у нас в проверке простоты)

### Race condition

Если просто добавить
```cpp
#pragma omp parallel for
```
Перед циклом, то мы будем получать рандомный результат (race condition, когда два треда иногда будут случайно изменять counter)
Кроме того, если какой-то старый тред, который долго спал возьмет и запишет свое значение в counter, то counter резко уменьшится.

Race condition возьникает, когда мы работаем с одной переменной из нескольких тредов.
Не бывает Race condition только на чтении (глабальная константа)

Как чинить?
#### Atomics

Обеспечивается одновременно одиночное выполнение операции среди всех потоков (только один тред может менять одновременно)

Проблемы
1. Не бесплатно!
2. Не любое действие можно сделать атомарным
3. Атомарны могут быть только одиночные действия -- например, вставка в двусвязный список

```cpp
#pragma omp atomic
counter++
```

#### Critical

Критический регион -- однотредовый регион многотредовой программы 

Когда один тред туда заходит, то другой ждет, пока первый оттуда выйдет

Проблемы:
1. Еще дороже чем atmoics (возомжно даже вызов ядра ОС), может использовать несколько атомарных операций под копотом
2. Противоречит идее многопоточности -- прога с такими секциями может даже быть дольше, чем однопоточная (из-за цены на вызов секций)


```cpp
#pragma omp critical 
counter++
```

#### Можно просто сделать частные переменные, которые постфактум суммируются

```cpp
#pragma omp parallel
{
    int counter2 = 0;
    #pragma omp for
    for (...)
        ...
        for(...)
            counter2++
        
        #pragma omp atomic
        counter+=counter2;
}
```

> Race condition -- это очень плохая ошибка -- ее сложно заметить